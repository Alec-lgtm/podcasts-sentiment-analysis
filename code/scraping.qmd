---
title: "vox_scraping"
format: html
editor: visual
---

```{r}
library(tidyverse)
library(rvest)
library(stringr)

page1 <- read_html("https://www.vox.com/explainers")

articles_titles <- page1 %>%
  html_elements(".qcd9z1") %>%
  html_text()

urls <- page1 %>%
  html_elements(".qcd9z1") %>%
  html_attr("href")

base_url <- "https://www.vox.com/"

full_url <- str_c(base_url, urls)

read_html(full_url[[1]]) %>%
html_elements("#zephr-anchor .xkp0cg0") %>%
html_text()



# scrapes all the article text in a vox article
scrape_articles <- function(article_url) {
  Sys.sleep(0.2)
  # print(read_html(article_url))
  
  # better to remove this from the list, it's more computationally efficient... ie full_url
  if(article_url == "https://www.vox.com/politics/389364/americas-ideological-fight-republican-democrat-explained" |
     article_url == "https://www.vox.com/future-perfect/386449/2024-future-perfect-50-progress-ai-climate-animal-welfare-innovation" |
     article_url == "https://www.vox.com/24066297/oscars-2024-guide-96th-academy-awards-what-to-watch-poor-things-oppenheimer-american-fiction" |
     article_url == "https://www.vox.com/politics/367990/kamala-harris-policy-positions-issues-guide" |
     article_url == "https://www.vox.com/policy/373288/the-right-explained" |
     article_url == "https://www.vox.com/politics/377783/democratic-party-kamala-harris-present-future" |
     article_url == "https://www.vox.com/politics/377783/democratic-party-kamala-harris-present-future" |
     article_url == "https://www.vox.com/even-better/385260/holiday-season-spending-money-pressure-hosting-guide" |
     article_url == "https://www.vox.com/politics/385161/the-rebuild-newsletter-sign-up-democratic-party-liberals-progressives")
    return()
  
  article_html <- read_html(article_url)
  
  article_text_vector <- article_html %>%
    html_elements("#zephr-anchor .xkp0cg1") %>%
    html_text()
  
  # browser()
  
  article_full_text <- paste(article_text_vector, collapse = " ")
  # print(article_full_text)
  
  article_titles <- article_html %>%
    html_elements(".xkp0cg9") %>%
    html_text()
  
  article_title <- article_titles[[1]]
  
  print(article_title) # debugging
  
  article_date <- article_html %>%
    html_elements("time") %>%
    html_text()
  
  # print(article_title)
  final_article_date <- article_date[[1]]
  cleaned_date_string <- str_replace(final_article_date, " UTC", "")
  article_date <- parse_date_time(cleaned_date_string, orders = "b d, Y, I:M p")
  
  return(tibble(
    url = article_url,
    title = article_title,
    datetime = article_date,
    text = article_full_text
  ))
}

test <- scrape_articles("https://www.vox.com/policy/390309/maha-rfk-make-america-healthy-again-slippery")

test1 <- scrape_articles("https://www.vox.com/politics/373376/trump-loss-2024-election-republican-party")

scrape_articles("https://www.vox.com/politics/390953/the-onion-infowars-alex-jones")
test <- scrape_articles("https://www.vox.com/politics/390108/working-class-definition-voters-2024")

scrape_page("https://www.vox.com/archives/2024/1/1")

# Goes through vox/archives/# to and scrapes all the page
scrape_page <- function(page_url) {
  Sys.sleep(0.2)
  page <- read_html(page_url)
  
  print(page_url)
  
  articles_titles <- page %>%
    html_elements(".qcd9z1") %>%
    html_text()
  
  urls <- page %>%
    html_elements(".qcd9z1") %>%
    html_attr("href")
  
  print(urls)
  
  urls <- str_sub(urls, 2)
  # browser()
  print(length(urls))
  
  full_url <- str_c(base_url, urls)
  
  print(class(full_url))
  
  scraped_urls <- character()
  
  scraped_articles <- list()
  # scraped_article <- vector("list", 15)
  
  # for (i in 1:length(urls)) {
  #   scraped_articles[[i]] <- scrape_articles(full_url[[i]])
  #   # print(full_url[[i]])
  # }
  
  # Loop through the URLs
  for (i in seq_along(full_urls)) {
    current_url <- full_url[[i]]
    
    # print(current_url)
    
    # Check if the URL has already been scraped
    if (!(current_url %in% scraped_urls)) {
      # Scrape the article and store it
      scraped_articles[[length(scraped_articles) + 1]] <- scrape_articles(current_url)
      
      # Add the URL to the list of scraped URLs
      scraped_urls <- c(scraped_urls, current_url)
    } else {
      message("Already scraped: ", current_url)
    }
  }
  
  return(scraped_articles)
}

scrape_month <-function(month_number) {
  #Sys.sleep(0.3)
  print(paste0("month number: ", month_number))
  page_url <- paste0(base_url, "archives/2024/", month_number, "/")
  page <- read_html(page_url)

  pagination_text <- page %>%
    html_element(".so8yiu0") %>%
    html_text()
  
  page_last_number <- str_extract(pagination_text, "\\d+(?=Next)")
  
  scraped_pages <- list()
  
  # for (i in 1:page_last_number) {
  #   scraped_pages[[i]] <- scrape_page(page_url)
  # }
  
  # scraped_pages <- map(1:page_last_number, ~ scrape_page(page_url)) %>%
  #   list_rbind()
  
  scraped_pages <- map(1:page_last_number, ~ scrape_page(paste0(page_url, .x))) %>%
    flatten() %>%
    list_rbind()
  
  return(scraped_pages)
}
# hello<- scrape_month(12)

# test_m <- scrape_month(2)

# check <- scrape_month(11)

# Run through 15 pages of the archive

the_big_one <- map(1:12,scrape_month) %>% 
  list_rbind()

saveRDS(the_big_one, file = "2024-2025_All_Vox_Articles.rds")

check_the_big_one <- the_big_one %>%
  list_rbind()

scraped_months <- list()
for (i in 1:12) {
  # print(scrape_page(i))
  scraped_pages[[i]] <- scrape_page(i)
}


# Fix page 6

saveRDS(scraped_pages, file = "scraped_pages_with_date_title_duplicates.rds")

```


```{r}

# Filtered podcast
filtered_data <- the_big_one %>%
  filter(!str_detect(text, "Today, Explained"))

write.csv(filtered_data, file = "vox_articles.csv", row.names = FALSE)

getwd()


# Define a function to extract 'like' with surrounding context
extract_context <- function(text) {
  matches <- str_extract_all(text, ".{0,30}\\blike\\b.{0,30}", simplify = TRUE)
  matches[matches != ""]
}

extract_context <- function(text) {
  matches <- str_extract_all(text, ".{0,30}\\bToday, Explained\\b.{0,30}", simplify = TRUE)
  matches[matches != ""]
}

# Apply the function to the 'text' column and create a new column for context
data_with_context <- the_big_one %>%
  mutate(context = sapply(text, extract_context))

# Unnest the context column if multiple matches exist per row
context_df <- data_with_context %>%
  select(url, title, datetime, context) %>%
  unnest(context)

context_df %>%
  select(context)
```


```{r}
library(RSelenium)
library(rvest)

system("pkill -f 'java -jar selenium'", ignore.stderr = TRUE)
system("pkill -f geckodriver", ignore.stderr = TRUE)
system("pkill -f firefox", ignore.stderr = TRUE)

# Start RSelenium
rD <- rsDriver(browser = "firefox", port = 4545L, verbose = FALSE)
remDr <- rD$client

# Navigate to the Vox website
remDr$navigate("https://www.vox.com/explainers")

page_source <- remDr$getPageSource()[[1]]

page <- read_html(page_source)

page1 <- read_html("https://www.vox.com/explainers")

articles_titles <- page1 %>%
  html_elements(".qcd9z1") %>%
  html_text()

urls <- page1 %>%
  html_elements(".qcd9z1") %>%
  html_attr("href")

base_url <- "https://www.vox.com/"

full_url <- str_c(base_url, urls)

articles <- page %>%
  html_elements("h2.c-entry-box--compact__title a") %>%
  html_attr("href") %>%
  tibble(link = .) %>%
  mutate(title = page %>%
           html_elements("h2.c-entry-box--compact__title a") %>%
           html_text())

  read_html(full_url[[1]]) %>%
html_elements("#zephr-anchor .xkp0cg0") %>%
 html_text()
# str_collapse()
# column bind

#zephr-anchor .xkp0cg0

#zephr-anchor .xkp0cg0


# remDr$navigate("https://www.facebook.com/")
# 
# remDr$navigate("https://www.nytimes.com/spotlight/podcasts")

# Wait for the page to load
Sys.sleep(5)

# Locate the article element by XPath
article_xpath <- "//div[contains(@class, 'duet--content-cards--content-card')]//a[contains(@href, '/israel/387191/icc-arrest-warrants-netanyahu')]"

# Find the article link and click on it
article_element <- remDr$findElement(using = "xpath", value = article_xpath)
article_element$clickElement()

# Wait for the article page to load
Sys.sleep(5)

# Scrape the article title
title_xpath <- "//h1[contains(@class, 'c-page-title')]"
title <- remDr$findElement(using = "xpath", value = title_xpath)$getElementText()

# Scrape the article content
content_xpath <- "//div[contains(@class, 'c-entry-content')]//p"
content_elements <- remDr$findElements(using = "xpath", value = content_xpath)

# Extract and combine text from all paragraphs
content <- sapply(content_elements, function(element) {
  element$getElementText()
})
content <- paste(unlist(content), collapse = "\n")

# Print the scraped data
cat("Title:", title, "\n\n")
cat("Content:", content, "\n")

# Close RSelenium
remDr$close()
rD$server$stop()

```

It seems that rvest only works for statically rendered websites.

```{r}
# Load required libraries
library(rvest)
library(dplyr)

# Define the URL for Vox
url <- "https://www.vox.com"

# Read the HTML content from the webpage
webpage <- read_html(url)

# Extract valid article links
article_links <- webpage %>%
  html_nodes("a") %>%
  html_attr("href") %>%
  na.omit() %>%
  # Filter for Vox articles using a regex for date-based URLs
  grep("^/\\d{4}/\\d{2}/\\d{2}/", ., value = TRUE) %>%
  unique() %>%
  paste0("https://www.vox.com", .)

# Extract article titles for those links
article_titles <- webpage %>%
  html_nodes("a") %>%
  html_text(trim = TRUE)

# Filter titles to match the valid links
filtered_titles <- article_titles[seq_along(article_links)]

# Create a data frame with titles and URLs
articles_df <- data.frame(
  Title = filtered_titles,
  URL = article_links,
  stringsAsFactors = FALSE
)

# Check if we have sufficient articles; sample randomly
if (nrow(articles_df) > 0) {
  set.seed(42) # For reproducibility
  sampled_articles <- articles_df %>%
    sample_n(min(431, nrow(articles_df)))
  
  # Print the sampled articles
  print(sampled_articles)
} else {
  message("No articles found. Check the website structure or adjust the scraping logic.")
}

```

So we gotta go with RSelenium

```{r}
# Install required packages if not already installed
if (!requireNamespace("RSelenium", quietly = TRUE)) {
  install.packages("RSelenium")
}
if (!requireNamespace("wdman", quietly = TRUE)) {
  install.packages("wdman")
}

# Function to set up RSelenium with Firefox
setup_rselenium <- function(port = 4445L) {
  # Check if Firefox is installed
  firefox <- wdman::selenium(retcommand = TRUE)
  
  # Start the Selenium server
  rD <- RSelenium::rsDriver(
    browser = "firefox",
    port = port,
    verbose = FALSE,
    chromever = NULL
  )
  
  return(rD)
}

# Usage example:
tryCatch({
  driver <- setup_rselenium()
  # Use the driver
  # driver$server$stop() # When done
}, error = function(e) {
  message("Error setting up RSelenium: ", e$message)
  message("Please ensure Firefox is installed and try again.")
})
```

```{r}
# Install and load required packages
if (!requireNamespace("RSelenium", quietly = TRUE)) install.packages("RSelenium")
if (!requireNamespace("wdman", quietly = TRUE)) install.packages("wdman")
library(RSelenium)
library(wdman)

# First, explicitly download and start the selenium server
selenium_object <- selenium(
  retcommand = FALSE,
  port = 4445L,
  version = "4.0.0",
  chromever = NULL,
  geckodriver = TRUE
)

# Then try to connect RSelenium
driver <- rsDriver(
  browser = "firefox",
  port = 4445L,
  verbose = TRUE,
  check = FALSE,
  selenium.dep = FALSE
)

# When done:
# driver$server$stop()
# selenium_object$stop()
```

```{r}
# Install and load required packages
library(RSelenium)
library(wdman)

# Start selenium with a specific version we know is available
selenium_server <- selenium(
  retcommand = FALSE,
  port = 4445L,
  version = "3.141.59",  # Using the latest stable version
  chromever = NULL,
  geckodriver = TRUE
)

# Then connect RSelenium
driver <- rsDriver(
  browser = "firefox",
  port = 4445L,
  verbose = TRUE,
  check = FALSE,
  selenium.dep = FALSE
)

# When done:
# driver$server$stop()
# selenium_server$stop()
```

```{r}
# Load required packages
library(RSelenium)

# First, check and kill any existing Selenium processes
system("pkill -f selenium", ignore.stderr = TRUE)
Sys.sleep(2)  # Give it time to clean up

# Create a Firefox driver instance directly
driver <- rsDriver(
  browser = "firefox",
  port = 4445L,
  version = "latest",
  geckover = "latest",  # Use latest geckodriver
  check = FALSE,
  verbose = TRUE
)

# Test the connection
remote_driver <- driver[["client"]]

# When done:
# driver$server$stop()
```

```{r}
rD <- rsDriver(browser = "firefox", port = 4445L, verbose = FALSE, 
               extraCapabilities = list(phantompath = "/Users/alec_chen/Sandbox/Tools"))

rD <- rsDriver(browser = "firefox", port = 4445L, verbose = FALSE)

Sys.getenv("PATH")

Sys.setenv(PATH = paste(Sys.getenv("PATH"), "/Users/alec_chen/Sandbox/Tools", sep = ":"))


```

```{r}
# Load required libraries
library(RSelenium)
library(rvest)
library(dplyr)

# Start RSelenium
rD <- rsDriver(browser = "firefox", port = 4445L, verbose = FALSE,
               )
remDr <- rD$client

# Navigate to the Vox homepage
remDr$navigate("https://www.vox.com")

# Let the page load
Sys.sleep(5)

# Get the page source after JavaScript has rendered it
page_source <- remDr$getPageSource()[[1]]

# Parse the page source with rvest
webpage <- read_html(page_source)

# Extract article links
article_links <- webpage %>%
  html_nodes("a") %>%
  html_attr("href") %>%
  na.omit() %>%
  grep("^/\\d{4}/\\d{2}/\\d{2}/", ., value = TRUE) %>%
  unique() %>%
  paste0("https://www.vox.com", .)

# Extract article titles
article_titles <- webpage %>%
  html_nodes("a") %>%
  html_text(trim = TRUE)

# Create data frame
articles_df <- data.frame(
  Title = article_titles[seq_along(article_links)],
  URL = article_links,
  stringsAsFactors = FALSE
)

# Take a random sample of 431 articles
set.seed(42)
sampled_articles <- articles_df %>%
  sample_n(min(431, nrow(articles_df)))

# Print sampled articles
print(sampled_articles)

# Close RSelenium
remDr$close()
rD$server$stop()

```
